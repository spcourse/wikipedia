# Data Acquisition

Data acquisition is the process of collecting or sampling data from a source. In the field of Computer Science, this often means downloading a database from an online source. However, not all websites provide an option to download the data displayed on their websites, as seen on platforms like Wikipedia or Twitter.

Web scraping is a way of extracting data from these websites. While this process could be done manually - by reading the information from the website, and then copying it information to a file - automation is much more efficient. Scraping can be a valuable tool for extracting data from websites quickly and systematically.

To be able to effectively scrape data from websites, a decent understanding of HTML is required. This module begins with an introduction to HTML.

Then, weâ€™ll dive into scraping data specifically from Wikipedia, demonstrating practical applications of scraping.

In the final part of the module, we will focus on web crawling. Web crawling is a technique where a program visits multiple webpages and scrapes each page for information. Search engines, for example, commonly use web crawling to index and update their content. In our case, we will use it to scrape data from various pages.
