# Data Acquisition

Data acquisition is the process of collecting or sampling data from a source. In the context of Computer Science, this often means downloading a database from a source on the internet. However, sometimes a website might not give an option to download the data, but does present it (think of websites like Wikipedia or Twitter).

Web scraping is a way of extracting data from websites. While this process could be done manually (by reading information on a website, and then copying that information to a file) it is usually done through the use of software. Scraping can be a valuable tool for extracting data.

Getting information from websites require a decent understanding of HTML, so we start of with that.

Then we'll focus on scraping data from Wikipedia.

In the final part of the module, we will focus on web crawling. Web crawling is a technique where a program visits multiple webpages and scrapes the page for information. An example of where this technique is used, is by search engines to update their web content. In our case, we will use it to scrape information from different pages.
